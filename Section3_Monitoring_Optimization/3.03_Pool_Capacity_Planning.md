# 3.3 Pool Capacity Planning（プール容量計画）

> **試験ガイド対応**: Section 4.3.3 - Pool capacity planning

---

## Objective: Monitor and adjust Pool settings based on resource constraints（リソース制約に基づくプール設定の監視と調整）

### プール容量計画の概要

プール容量計画は、Horizon 環境のユーザー数、ワークロードタイプ、SLA 要件に基づいて、必要な vCPU、vRAM、ストレージ、ネットワーク帯域幅を見積もり、適切なプールサイズとプロビジョニング設定を構成するプロセス。Projected（計画値）と Actual（実測値）のギャップを継続的に監視し、リソース不足やオーバープロビジョニングを防止する。

---

## 1. vCPU の見積もり

### ユーザータイプ別 vCPU 推奨値

| ユーザータイプ | vCPU | 根拠 |
|---|---|---|
| タスクワーカー（データ入力、コールセンター） | 2 vCPU | 単一アプリケーション使用、低 CPU 要件 |
| ナレッジワーカー（Office、Web、メール） | 2-4 vCPU | 複数アプリケーション同時使用 |
| パワーユーザー（開発者、アナリスト） | 4-8 vCPU | コンパイル、データ分析等の高 CPU 要件 |
| GPU ワーカー（CAD、3D モデリング） | 4-8 vCPU + vGPU | GPU 処理と併用する CPU リソース |
| RDSH セッションホスト | 8-16 vCPU | 複数ユーザーセッションのホスティング |

### vCPU 計算式

**物理ホストあたりの VM 集約数**:

```
VM集約数 = (物理コア数 × CPU オーバーコミット比) ÷ VM あたりの vCPU 数
```

| パラメータ | タスクワーカー | ナレッジワーカー | パワーユーザー |
|---|---|---|---|
| CPU オーバーコミット比 | 8:1 - 10:1 | 6:1 - 8:1 | 3:1 - 5:1 |
| VM あたり vCPU | 2 | 2-4 | 4-8 |

**例**: 物理ホスト（2 ソケット × 16 コア = 32 コア）にナレッジワーカー（2 vCPU）を配置する場合:

```
VM集約数 = (32 × 6) ÷ 2 = 96 VM/ホスト
```

> **重要**: CPU オーバーコミット比はユーザータイプに依存する。タスクワーカーは高いオーバーコミット（8:1-10:1）、パワーユーザーは低いオーバーコミット（3:1-5:1）が推奨される。

---

## 2. vRAM の見積もり

### ユーザータイプ別 vRAM 推奨値

| ユーザータイプ | vRAM | 根拠 |
|---|---|---|
| タスクワーカー | 2-4 GB | 最小限のアプリケーションフットプリント |
| ナレッジワーカー | 4-8 GB | Office + ブラウザ + 追加アプリ |
| パワーユーザー | 8-16 GB | 大規模データセット処理、IDE 使用 |
| GPU ワーカー | 8-32 GB | 3D モデル・テクスチャデータのメモリ要件 |
| RDSH セッションホスト | 24-64 GB | 複数ユーザーセッション（4 GB/セッション × セッション数） |

### vRAM 計算式

**ホストあたり必要物理メモリ**:

```
必要メモリ = (VM数 × VMあたりのvRAM) + ESXiオーバーヘッド + vSphereメモリ予約
```

| パラメータ | 値 | 説明 |
|---|---|---|
| ESXi オーバーヘッド | 4-8 GB | ESXi カーネルとエージェントのメモリ |
| vSphere メモリ予約 | VM 数に依存 | VM ごとのメモリオーバーヘッド（約 50-100 MB/VM） |

**例**: 96 VM × 4 GB = 384 GB + 8 GB (ESXi) + 9.6 GB (100 MB × 96) = 401.6 GB

> **注意**: メモリのオーバーコミットは VDI 環境では推奨されない。ページスワップによるパフォーマンス低下がユーザーエクスペリエンスに直結する。

---

## 3. IOPS の見積もり

### ユーザータイプ別 IOPS 推奨値

| ユーザータイプ | 定常状態 IOPS | ピーク IOPS | 説明 |
|---|---|---|---|
| タスクワーカー | 5-10 IOPS | 15-20 IOPS | 軽量なディスク I/O |
| ナレッジワーカー | 10-20 IOPS | 30-50 IOPS | 中程度のドキュメント操作 |
| パワーユーザー | 20-40 IOPS | 50-100 IOPS | 大量のファイルアクセス |
| ログオンストーム | 50-100 IOPS/VM | - | 始業時の一斉ログオン |
| ブートストーム | 50-100 IOPS/VM | - | インスタントクローンの一斉プロビジョニング |

### IOPS 計算式

**プール全体の必要 IOPS**:

```
定常状態IOPS = VM数 × VMあたりの定常IOPS
ピークIOPS = VM数 × VMあたりのピークIOPS × 同時ピーク率
ログオンストームIOPS = 同時ログオンVM数 × ログオンIOPS/VM
```

**ストレージ設計の基準 IOPS**:

```
設計IOPS = MAX(ピークIOPS, ログオンストームIOPS) × 安全マージン(1.2)
```

### ストレージタイプ別 IOPS 性能目安

| ストレージタイプ | IOPS/ディスク | 特徴 |
|---|---|---|
| SAS 10K RPM | 130-150 | 従来型、コスト重視 |
| SAS 15K RPM | 175-200 | 従来型、高性能 |
| SSD（SATA） | 5,000-50,000 | 高IOPS、低レイテンシ |
| NVMe SSD | 100,000-500,000+ | 最高性能、vSAN に最適 |
| vSAN（ハイブリッド） | キャッシュ層に依存 | SSD キャッシュ + HDD 容量層 |
| vSAN（オールフラッシュ） | 高IOPS | 全SSD構成 |

---

## 4. ストレージ容量の見積もり

### デスクトップ VM のディスク構成

| ディスクタイプ | インスタントクローン | フルクローン | 説明 |
|---|---|---|---|
| **OS ディスク** | 差分ディスク（数GB） | フルコピー（40-80 GB） | OS とアプリケーション |
| **ユーザーデータ** | 永続ディスク（オプション） | VM 内蔵 | ユーザープロファイル・データ |
| **ページファイル** | 一時ディスク（1-4 GB） | VM 内蔵 | Windows ページファイル |
| **レプリカディスク** | 共有（40-80 GB × レプリカ数） | なし | ゴールデンイメージのベース |

### インスタントクローンのストレージ計算

```
プールストレージ = レプリカサイズ × レプリカ数 + (VM数 × 差分ディスクサイズ) + (VM数 × 一時ディスクサイズ)
```

**例**: 500 VM のインスタントクローンプール

```
レプリカ: 60 GB × 2（データストア冗長）= 120 GB
差分ディスク: 500 × 5 GB（平均）= 2,500 GB
一時ディスク: 500 × 2 GB = 1,000 GB
合計: 120 + 2,500 + 1,000 = 3,620 GB ≒ 3.6 TB
```

---

## 5. ネットワーク帯域幅の見積もり

### プロトコル別帯域幅消費

| プロトコル | タスクワーカー | ナレッジワーカー | マルチメディア | GPU ワーカー |
|---|---|---|---|---|
| **Blast（LAN）** | 100-300 Kbps | 500-1,500 Kbps | 2-5 Mbps | 5-20 Mbps |
| **Blast（WAN）** | 50-200 Kbps | 300-800 Kbps | 1-3 Mbps | 3-10 Mbps |

### ネットワーク帯域幅計算

```
必要帯域幅 = 同時接続ユーザー数 × ユーザーあたりの平均帯域幅 × 安全マージン(1.3)
```

### Teams 最適化の帯域幅への影響

Media Optimization for Microsoft Teams が有効な場合、Teams のメディアトラフィックはクライアント端末から直接送受信されるため、データセンターのネットワーク帯域幅には影響しない。ただし、クライアント端末のインターネット帯域幅を考慮する必要がある。

Fallback モード（RTAV）の場合、Teams のメディアトラフィックは仮想デスクトップ経由で処理されるため、Blast セッション帯域幅に追加で 1-3 Mbps のオーバーヘッドが発生する。

---

## 6. インフラストラクチャコンポーネントのリソース要件

### Horizon インフラ VM スペック（Reference Architecture）

| コンポーネント | vCPU | vMemory | ディスク | スケーリング |
|---|---|---|---|---|
| **Connection Server** | 4 | 12 GB | 100 GB | 最大 2,000 セッション/サーバー（推奨） |
| **Enrollment Server** | 4 | 12 GB | 100 GB | True SSO 用 |
| **SQL Server (Horizon)** | 2 | 8 GB | 50 GB (OS) + 50 GB (Data) | イベントDB、App Volumes DB |
| **App Volumes Manager** | 2 | 8 GB | 100 GB | - |
| **UAG（小規模）** | 2 | 4 GB | - | 最大 2,000 セッション |
| **UAG（大規模）** | 4 | 8-16 GB | - | 最大 10,000 セッション |

### Connection Server のスケーリング

| 項目 | 値 |
|---|---|
| 1 CS あたりの推奨セッション数 | 2,000 |
| Pod あたりの最大 CS 数 | 8 |
| Pod あたりの最大セッション数 | 12,000 |
| CPA（Cloud Pod Architecture）最大セッション数 | 250,000（25 Pod） |

---

## 7. プール設定と容量管理

### Automated Desktop Pool の主要設定

| 設定 | 説明 | 容量計画への影響 |
|---|---|---|
| **Max Number of Machines** | プール内の最大 VM 数 | ホストリソースの上限を規定 |
| **Spare (Powered On) Machines** | 事前起動済みの予備 VM 数 | ユーザー待ち時間の短縮とリソース消費のバランス |
| **Provisioning Rate** | 同時プロビジョニング数 | ストレージ IOPS とネットワーク帯域幅に影響 |
| **Minimum Number of Machines** | 最小 VM 数（プール縮小の下限） | 常に確保するリソースの最小値 |

### Spare（予備 VM）の設計

Spare VM はユーザーが接続要求した際に即座に割り当てられる事前プロビジョニング済み VM。

**Spare VM 数の決定要因**:

| 要因 | 考慮事項 |
|---|---|
| **ピーク接続率** | 始業時の同時接続数/分を計測し、プロビジョニング速度で追いつかない分を Spare で補う |
| **プロビジョニング時間** | インスタントクローンの作成時間（通常 1-2 分）を考慮 |
| **SLA 要件** | ユーザーの接続待ち時間の許容値 |
| **リソースコスト** | Spare VM は電源オン状態のためリソースを消費する |

**Spare VM 数の計算式**:

```
Spare VM数 = ピーク接続率(VM/分) × プロビジョニング時間(分) × 安全マージン
```

**例**: ピーク時に 30 VM/分の接続、プロビジョニング時間 2 分、安全マージン 1.5:

```
Spare VM数 = 30 × 2 × 1.5 = 90 VM
```

### Provisioning Rate（プロビジョニングレート）の調整

Provisioning Rate は同時にプロビジョニングできる VM 数を制御する。

| 考慮事項 | 影響 |
|---|---|
| **ストレージ IOPS** | プロビジョニング時に大量のディスク I/O が発生。IOPS が不足するとプロビジョニングが遅延 |
| **ネットワーク帯域幅** | レプリカディスクのコピーやカスタマイズスクリプトの実行に帯域を消費 |
| **vCenter の処理能力** | 同時操作が多いと vCenter のタスクキューが圧迫される |
| **ホスト CPU/メモリ** | 同時起動する VM 数が多いとホストリソースが一時的に不足 |

**調整の指針**:

| シナリオ | Provisioning Rate | 根拠 |
|---|---|---|
| オールフラッシュストレージ | 高め（8-20） | 高 IOPS で並行処理に耐えられる |
| ハイブリッドストレージ | 中程度（4-8） | IOPS ボトルネックを避ける |
| HDD ストレージ | 低め（2-4） | IOPS 制約により並行数を制限 |
| ログオンストーム対策 | Spare VM を増やす | Rate を上げるよりも事前プロビジョニングが効果的 |

---

## 8. Projected vs Actual の監視

### Projected（計画値）と Actual（実測値）の比較

| 指標 | Projected（計画値） | Actual（実測値） | 監視方法 | 閾値 |
|---|---|---|---|---|
| **同時セッション数** | SLA/契約に基づく想定値 | Horizon Console のダッシュボード | Dashboard > Sessions | プール使用率 80% 超で拡張検討 |
| **CPU 使用率** | オーバーコミット比から算出 | vCenter / vROps のホストメトリクス | ホストレベルで監視 | 80% で警告、90% で要対応 |
| **メモリ使用率** | VM数 × vRAM + オーバーヘッド | vCenter / vROps のホストメトリクス | ホストレベルで監視 | 90% で警告（スワップ発生前に対応） |
| **ストレージ IOPS** | ユーザータイプ別の計算値 | vCenter / vSAN パフォーマンスモニタ | データストアレベル | レイテンシ 10ms 超で調査 |
| **ストレージ容量** | 差分ディスク成長率の予測 | vCenter データストア使用率 | データストアレベル | 80% で警告 |
| **ネットワーク帯域幅** | ユーザー数 × プロトコル帯域幅 | ネットワークモニタリング / vROps | リンクレベル | リンク使用率 70% で警告 |
| **プール使用率** | Max Machines に対する割合 | Horizon Console > Pool 詳細 | プールレベル | 80% 超で拡張検討 |

### Horizon Console での監視ポイント

| 監視項目 | 場所 | 確認内容 |
|---|---|---|
| **セッション数** | Dashboard > Sessions | 現在のアクティブセッション、切断セッション数 |
| **マシンの状態** | Inventory > Machines | Available / Connected / Provisioning / Error の状態別台数 |
| **プール使用率** | Inventory > Desktops | 割り当て済み / 未割り当て / Spare の台数 |
| **問題のあるマシン** | Monitor > Dashboard | エラー状態のマシン、プロビジョニング失敗 |
| **イベント** | Monitor > Events | プロビジョニングイベント、エラーイベント |

### リソース監視のワークフロー

容量管理は以下のサイクルで継続的に実施する:

```
計画(Plan) → 監視(Monitor) → 分析(Analyze) → 調整(Adjust) → 計画(Plan)
```

| フェーズ | 活動 |
|---|---|
| **計画** | ユーザータイプ別のリソース要件を見積もり、プール設定（Max/Min/Spare）を決定 |
| **監視** | Horizon Console、vCenter、vROps/Omnissa Intelligence でリアルタイムメトリクスを収集 |
| **分析** | Projected vs Actual のギャップを特定。トレンド分析で将来の容量需要を予測 |
| **調整** | プール設定の変更、ホストの追加/削除、ストレージの拡張を実施 |

### Omnissa Intelligence による容量分析

Omnissa Intelligence は Horizon 環境のデータを集約・分析し、容量計画を支援するクラウドベースの分析プラットフォーム。

| 機能 | 説明 |
|---|---|
| **ダッシュボード** | セッション数、リソース使用率、エラー率の可視化 |
| **トレンド分析** | 過去のデータに基づく使用パターンの分析と将来需要の予測 |
| **アラート** | 閾値超過時の自動通知 |
| **レポート** | 定期的な容量レポートの生成 |
| **ETL Connector** | オンプレミス環境からのデータ収集（Intelligence Collection Service） |

> **補足**: Omnissa Intelligence は Workspace ONE Intelligence プラットフォームの一部であり、Horizon 環境のメトリクスを収集するには ETL Connector Server（4 vCPU, 8 GB RAM）のデプロイが必要。

### 容量不足の兆候と対応

| 兆候 | 原因の可能性 | 対応策 |
|---|---|---|
| ユーザーが接続できない | Spare VM 不足、Max Machines に到達 | Spare VM 数の増加、Max Machines の引き上げ |
| ログオンが遅い | ストレージ IOPS 不足、ログオンストーム | Provisioning Rate の調整、Spare VM の増加、ストレージ増強 |
| セッション応答が遅い | CPU オーバーコミット過多、メモリスワップ | ホストの追加、VM 配置の再調整 |
| プロビジョニング失敗 | ストレージ容量不足、vCenter のタスク輻輳 | ストレージ追加、Provisioning Rate の引き下げ |
| 差分ディスクの急成長 | アプリケーションの過剰なディスク書き込み | アプリケーションの最適化、ページファイルの固定サイズ化 |

### プール設定の調整手順

容量不足が検出された場合の調整フロー:

| ステップ | 操作 | 確認事項 |
|---|---|---|
| 1. 現状把握 | Horizon Console で現在のプール状態を確認 | Available / Error / Provisioning の VM 数 |
| 2. ボトルネック特定 | vCenter でホストリソースを確認 | CPU Ready Time、メモリスワップ、ストレージレイテンシ |
| 3. 短期対応 | Spare VM の増加、Max Machines の引き上げ | リソース上限を超えていないか |
| 4. 中期対応 | ホスト追加、ストレージ拡張 | N+1 冗長性の維持 |
| 5. 長期対応 | プール構成の見直し、ワークロード分離 | ユーザータイプ別のプール分割 |

---

## 9. ホスト障害に対する N+1 設計

### N+1 冗長性の計算

**ホスト台数の計算**:

```
必要ホスト台数 = CEIL(必要VM数 ÷ ホストあたりVM集約数) + 1
```

**例**: 500 VM、ホストあたり 96 VM の場合:

```
必要ホスト台数 = CEIL(500 ÷ 96) + 1 = 6 + 1 = 7 ホスト
```

障害時の動作:
- vSphere HA が障害ホストの VM を残りのホストで再起動
- N+1 設計により、1 ホスト障害時でも全 VM の稼働を保証
- Admission Control で HA スロットを確保

---

## 10. RDSH プール容量計画

### RDSH セッション密度の計算

```
ホストあたりセッション数 = (ホストvCPU × CPU使用効率) ÷ セッションあたりCPU消費
```

RDSH セッション密度は、RDSH VM あたりのセッション数とホストあたりの RDSH VM 数の掛け算で決定される。

| パラメータ | タスクワーカー | ナレッジワーカー |
|---|---|---|
| RDSH VM あたりセッション数 | 25-50 | 15-25 |
| RDSH VM vCPU | 8-16 | 8-16 |
| RDSH VM vRAM | 24-48 GB | 32-64 GB |
| ユーザーあたり vRAM | 1-2 GB | 2-4 GB |

---

## ベストプラクティス

1. **パイロットテストでベースラインを取得**: 本番展開前に 10-20% のユーザーでパイロットを実施し、実際のリソース消費パターンを計測する
2. **ログオンストームへの備え**: 始業時の同時接続ピークに対して、十分な Spare VM と IOPS を確保する
3. **N+1 設計の採用**: ホスト障害時にもユーザーへの影響を最小化するため、最低 1 ホスト分の余裕を確保
4. **メモリのオーバーコミットを避ける**: VDI ではメモリスワップがユーザーエクスペリエンスに直結するため、物理メモリ内に収める
5. **定期的な Projected vs Actual レビュー**: 月次でリソース使用トレンドを分析し、3-6 ヶ月先の容量を予測する
6. **Provisioning Rate とストレージの整合性**: ストレージの IOPS キャパシティに合わせて Provisioning Rate を調整する
7. **差分ディスクの成長率を監視**: インスタントクローンの差分ディスクが想定以上に成長する場合、アプリケーションの挙動を調査
8. **Logon Monitor と連携**: ログオン時間が SLA を超過する場合、Logon Monitor のメトリクスでボトルネック（GPO / Profile / Shell）を特定する
9. **Teams 最適化の影響を考慮**: Media Optimization が有効な場合はデータセンター帯域幅は減少するが、Fallback モードのユーザーがいる場合は追加帯域幅を見込む

## アンチパターン

1. **Spare VM 数のゼロ設定**: ユーザー接続時にオンデマンドプロビジョニングが発生し、接続待ち時間が増大
2. **Provisioning Rate の過剰設定**: ストレージ IOPS を超える並行プロビジョニングにより、既存セッションのパフォーマンスが低下
3. **メモリのオーバーコミット**: スワップ発生による著しいパフォーマンス低下。TPS (Transparent Page Sharing) だけでは不十分
4. **CPU オーバーコミットの過剰**: Ready time（%RDY）の増加によりユーザーの操作応答が遅延
5. **Projected のみで運用し Actual を監視しない**: 実際の使用パターンが想定と異なり、リソース不足やオーバープロビジョニングに気付かない
6. **N+1 設計を省略**: 1 ホスト障害で大量のユーザーが接続不能になるリスク

---

## 理解度チェックリスト

### リソース見積もり
- [ ] ユーザータイプ別の vCPU、vRAM、IOPS の推奨値を説明できる
- [ ] CPU オーバーコミット比がユーザータイプによって異なる理由を理解している
- [ ] メモリのオーバーコミットが VDI で推奨されない理由を説明できる
- [ ] ログオンストーム/ブートストームの IOPS への影響を理解している
- [ ] インスタントクローンのストレージ構成（レプリカ、差分ディスク、一時ディスク）を説明できる

### プール設定
- [ ] Spare (Powered On) Machines の役割と適切な数の決定方法を説明できる
- [ ] Provisioning Rate の調整がストレージ IOPS に与える影響を理解している
- [ ] Max Number of Machines と Minimum Number of Machines の関係を理解している

### 監視と調整
- [ ] Projected vs Actual の主要な監視指標（CPU、メモリ、IOPS、セッション数）を列挙できる
- [ ] Horizon Console で確認すべき容量関連の項目を説明できる
- [ ] 容量不足の兆候（接続不可、ログオン遅延、プロビジョニング失敗）に対する対応策を判断できる
- [ ] N+1 設計の計算方法を理解している
- [ ] 容量管理のサイクル（計画→監視→分析→調整）を説明できる

### インフラストラクチャ
- [ ] Connection Server の推奨スペック（4 vCPU, 12 GB vRAM）を把握している
- [ ] Connection Server のスケーリング上限（Pod あたり最大 12,000 セッション）を理解している
- [ ] RDSH セッション密度の計算方法を説明できる
- [ ] Teams 最適化が容量計画に与える影響（帯域幅の変化）を理解している
